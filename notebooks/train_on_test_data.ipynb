{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aaanpilov/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/home/aaanpilov/.local/lib/python3.8/site-packages/xgboost/core.py:265: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc 2.28+) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import recall_score, classification_report\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "import numpy as np\n",
    "from fmri_processing.utils import draw_heat_map\n",
    "from fmri_processing.functions import funcs\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_matrix  = '/home/aaanpilov/diploma/project/numpy_matrixes/average_stimulus/HC/max.npy'\n",
    "# test_matrix = '/home/aaanpilov/diploma/project/numpy_matrixes/average_stimulus/test/max.npy'\n",
    "\n",
    "# train_matrix, test_matrix = test_matrix, train_matrix\n",
    "def draw_all_subjects(matrix):\n",
    "    N = matrix.shape[0]  # Длина массива\n",
    "    sub_num = N // 5\n",
    "\n",
    "    subjects = np.array_split(matrix, sub_num)\n",
    "    for idx, sub in enumerate(subjects):\n",
    "        print(f'sub-{idx:02d}')\n",
    "        draw_heat_map(subjects[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def z_score_matrix(matrix):\n",
    "    shape = matrix.shape\n",
    "\n",
    "    # 1. Разбиваем матрицу на группы по 5 элементов\n",
    "    flattened = matrix.flatten()  # Преобразуем в 1D-массив\n",
    "    num_groups = len(flattened) // 5\n",
    "    groups = flattened[:num_groups * 5].reshape(-1, 5)  # Группы по 5 элементов\n",
    "\n",
    "    # 2. Вычисляем z-показатели для каждой группы\n",
    "    z_scores = np.zeros_like(groups)\n",
    "    for i in range(groups.shape[0]):\n",
    "        group = groups[i]\n",
    "        mean = np.mean(group)\n",
    "        std = np.std(group)\n",
    "        if std != 0:\n",
    "            z_scores[i] = (group - mean) / std\n",
    "        else:\n",
    "            z_scores[i] = 0  # Если все элементы одинаковые\n",
    "\n",
    "    # 3. Собираем обратно в матрицу\n",
    "    flattened_z = z_scores.flatten()\n",
    "    # Если исходная длина не делилась на 5, добавляем оставшиеся элементы без изменений\n",
    "    if len(flattened) % 5 != 0:\n",
    "        remaining = flattened[num_groups * 5:]\n",
    "        flattened_z = np.concatenate([flattened_z, remaining])\n",
    "\n",
    "    # Преобразуем обратно в исходную размерность\n",
    "    result_matrix = flattened_z.reshape(shape)\n",
    "    return result_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(train_matrix):\n",
    "    matrix = np.load(train_matrix)\n",
    "    N = matrix.shape[0]  # Длина массива\n",
    "    sub_num = N // 5    # Количество испытуемых\n",
    "\n",
    "    labels = np.zeros(N, dtype=int)  # Создаем массив из нулей\n",
    "    labels[3::5] = 1  # Каждый 4-й элемен\n",
    "    print(matrix.shape)\n",
    "\n",
    "    X = matrix\n",
    "    y = labels\n",
    "\n",
    "\n",
    "    # Группы для разделения\n",
    "    groups = np.repeat(np.arange(sub_num), 5)  # [0,0,0,0,0, 1,1,1,1,1,...]\n",
    "\n",
    "    splitter = GroupShuffleSplit(n_splits=1, test_size=0.3, random_state=30)\n",
    "    train_idx, test_idx = next(splitter.split(X, y, groups))\n",
    "\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_best_model_by_recall_(train_matrix, target_class=1, test_size=0.3, random_state=42, verbose=True):\n",
    "    \"\"\"\n",
    "    Обучает модели и возвращает лучшую по recall для указанного класса\n",
    "    \n",
    "    Параметры:\n",
    "    X - признаки\n",
    "    y - целевая переменная\n",
    "    groups - группы для кросс-валидации\n",
    "    target_class - класс, для которого оптимизируем recall (по умолчанию 1)\n",
    "    test_size - доля тестовой выборки\n",
    "    random_state - для воспроизводимости\n",
    "    verbose - вывод информации о процессе обучения\n",
    "    \"\"\"\n",
    "    \n",
    "    matrix = np.load(train_matrix)\n",
    "    N = matrix.shape[0]  # Длина массива\n",
    "    sub_num = N // 5    # Количество испытуемых\n",
    "\n",
    "    labels = np.zeros(N, dtype=int)  # Создаем массив из нулей\n",
    "    labels[3::5] = 1  # Каждый 4-й элемен\n",
    "    print(matrix.shape)\n",
    "\n",
    "    X = matrix\n",
    "    y = labels\n",
    "\n",
    "\n",
    "    # Группы для разделения\n",
    "    groups = np.repeat(np.arange(sub_num), 5)  # [0,0,0,0,0, 1,1,1,1,1,...]\n",
    "\n",
    "    splitter = GroupShuffleSplit(n_splits=1, test_size=0.3, random_state=30)\n",
    "    train_idx, test_idx = next(splitter.split(X, y, groups))\n",
    "\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    \n",
    "    # 1. Разделение на train/test с сохранением групп\n",
    "    splitter = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
    "    train_idx, test_idx = next(splitter.split(X, y, groups))\n",
    "    \n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    groups_train = groups[train_idx]\n",
    "    \n",
    "    # 2. Инициализация моделей\n",
    "    models = {\n",
    "        \"Logistic Regression\": Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('model', LogisticRegression(class_weight='balanced', \n",
    "                                       max_iter=1000, \n",
    "                                       random_state=random_state))\n",
    "        ]),\n",
    "        \"Random Forest\": RandomForestClassifier(class_weight='balanced_subsample', \n",
    "                                              random_state=random_state),\n",
    "        \"SVM\": Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('model', SVC(kernel='rbf', \n",
    "                         class_weight='balanced', \n",
    "                         probability=True, \n",
    "                         random_state=random_state))\n",
    "        ]),\n",
    "        \"XGBoost\": XGBClassifier(\n",
    "            scale_pos_weight=4,  # Автоматический расчет\n",
    "            random_state=random_state\n",
    "        )\n",
    "    }\n",
    "\n",
    "    # 3. Кросс-валидация по группам\n",
    "    gss = GroupShuffleSplit(n_splits=5, test_size=0.3, random_state=random_state)\n",
    "    model_recalls = []\n",
    "\n",
    "    for name, model in models.items():\n",
    "        recall_scores = []\n",
    "        \n",
    "        for fold, (train_idx_fold, val_idx_fold) in enumerate(gss.split(X_train, y_train, groups_train)):\n",
    "            model.fit(X_train[train_idx_fold], y_train[train_idx_fold])\n",
    "            y_pred = model.predict(X_train[val_idx_fold])\n",
    "            recall = recall_score(y_train[val_idx_fold], y_pred, pos_label=target_class)\n",
    "            recall_scores.append(recall)\n",
    "        \n",
    "        print(recall_scores)\n",
    "        mean_recall = np.mean(recall_scores)\n",
    "        std_recall = np.std(recall_scores)\n",
    "        model_recalls.append((name, mean_recall, std_recall, model))\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"{name:<20} | Recall (class {target_class}): {mean_recall:.3f} ± {std_recall:.3f}\")\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "    # # 4. Выбор лучшей модели\n",
    "    # best_name, best_recall, best_std, best_model = max(model_recalls, key=lambda x: x[1])\n",
    "    \n",
    "    # # 5. Финальное обучение на полном train наборе\n",
    "    # best_model.fit(X_train, y_train)\n",
    "    \n",
    "    # # 6. Оценка на тестовом наборе\n",
    "    # test_recall = recall_score(y_test, best_model.predict(X_test), pos_label=target_class)\n",
    "    \n",
    "    # if verbose:\n",
    "    #     print(f\"\\n{'='*50}\")\n",
    "    #     print(f\"BEST MODEL: {best_name}\")\n",
    "    #     print(f\"CV Recall (class {target_class}): {best_recall:.3f} ± {best_std:.3f}\")\n",
    "    #     print(f\"Test Recall (class {target_class}): {test_recall:.3f}\")\n",
    "    #     print(\"=\"*50)\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_best_model_by_recall(train_matrix, target_class=1, test_size=0.3, random_state=42, verbose=True):\n",
    "    \"\"\"\n",
    "    Обучает модели и возвращает лучшую по recall для указанного класса\n",
    "    \n",
    "    Параметры:\n",
    "    train_matrix - путь к файлу с матрицей признаков\n",
    "    target_class - класс, для которого оптимизируем recall\n",
    "    test_size - доля валидационной выборки в кросс-валидации\n",
    "    random_state - для воспроизводимости\n",
    "    verbose - вывод информации о процессе обучения\n",
    "    \n",
    "    Возвращает:\n",
    "    Лучшую модель (по recall на валидации), метрики моделей\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Загрузка данных и подготовка\n",
    "    matrix = np.load(train_matrix)\n",
    "    N = matrix.shape[0]\n",
    "    sub_num = N // 5\n",
    "\n",
    "    labels = np.zeros(N, dtype=int)\n",
    "    labels[3::5] = 1\n",
    "    \n",
    "    X = matrix\n",
    "    y = labels\n",
    "    groups = np.repeat(np.arange(sub_num), 5)\n",
    "\n",
    "    # 2. Инициализация моделей\n",
    "    models = {\n",
    "        \"Logistic Regression\": Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('model', LogisticRegression(class_weight='balanced', \n",
    "                                       max_iter=1000, \n",
    "                                       random_state=random_state))\n",
    "        ]),\n",
    "\n",
    "\n",
    "\n",
    "    }\n",
    "    # from sklearn.model_selection import RandomizedSearchCV\n",
    "    # from scipy.stats import loguniform  # для логарифмического распределения C\n",
    "\n",
    "    # param_dist = {\n",
    "    #     'model__C': loguniform(1e-4, 100),  # C в диапазоне [0.0001, 100]\n",
    "    #     'model__penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "    #     'model__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "    #     'model__l1_ratio': [0, 0.25, 0.5, 0.75, 1]  # для elasticnet\n",
    "    # }\n",
    "\n",
    "    # random_search = RandomizedSearchCV(\n",
    "    #     estimator=models['Logistic Regression'],\n",
    "    #     param_distributions=param_dist,\n",
    "    #     n_iter=50,  # количество случайных комбинаций\n",
    "    #     scoring='accuracy',\n",
    "    #     cv=5,\n",
    "    #     n_jobs=-1,\n",
    "    #     verbose=1,\n",
    "    #     random_state=random_state\n",
    "    # )\n",
    "\n",
    "    # models['Logistic Regression'] = random_search\n",
    "\n",
    "    # 3. Кросс-валидация по группам\n",
    "    gss = GroupShuffleSplit(n_splits=5, test_size=test_size, random_state=random_state)\n",
    "    model_recalls = []\n",
    "\n",
    "    for name, model in models.items():\n",
    "        recall_scores = []\n",
    "        \n",
    "        for train_idx, val_idx in gss.split(X, y, groups):\n",
    "            model.fit(X[train_idx], y[train_idx])\n",
    "            y_pred = model.predict(X[val_idx])\n",
    "            recall = recall_score(y[val_idx], y_pred, pos_label=target_class)\n",
    "            recall_scores.append(recall)\n",
    "        \n",
    "        mean_recall = np.mean(recall_scores)\n",
    "        std_recall = np.std(recall_scores)\n",
    "        model_recalls.append((name, mean_recall, std_recall, model))\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"{name:<20} | Recall (class {target_class}): {mean_recall:.3f} ± {std_recall:.3f}\")\n",
    "        model.fit(X, y)\n",
    "    # # 4. Выбор лучшей модели\n",
    "    # best_model_info = max(model_recalls, key=lambda x: x[1])\n",
    "    # best_model = best_model_info[3]\n",
    "    \n",
    "    # # 5. Финальное обучение на всех данных\n",
    "    # best_model.fit(X, y)\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "def train_best_model_by_recall(train_matrix, target_class=1, random_state=42, verbose=True):\n",
    "    \"\"\"\n",
    "    Обучает модели и возвращает лучшую по recall для указанного класса\n",
    "    \n",
    "    Параметры:\n",
    "    train_matrix - путь к файлу с матрицей признаков\n",
    "    target_class - класс, для которого оптимизируем recall\n",
    "    random_state - для воспроизводимости\n",
    "    verbose - вывод информации о процессе обучения\n",
    "    \n",
    "    Возвращает:\n",
    "    Лучшую модель (по recall на валидации), метрики моделей\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Загрузка данных и подготовка\n",
    "    matrix = np.load(train_matrix)\n",
    "    N = matrix.shape[0]\n",
    "    sub_num = N // 5\n",
    "\n",
    "    labels = np.zeros(N, dtype=int)\n",
    "    labels[3::5] = 1\n",
    "    \n",
    "    X = matrix\n",
    "    y = labels\n",
    "    groups = np.repeat(np.arange(sub_num), 5)\n",
    "\n",
    "    # 2. Инициализация моделей\n",
    "    models = {\n",
    "        \"Logistic Regression\": Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('model', LogisticRegression(class_weight='balanced', \n",
    "                                       max_iter=1000, \n",
    "                                       random_state=random_state))\n",
    "        ]),\n",
    "        \"Random Forest\": RandomForestClassifier(\n",
    "            class_weight='balanced_subsample', \n",
    "            random_state=random_state\n",
    "        ),\n",
    "        \"SVM\": Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('model', SVC(kernel='rbf', \n",
    "                         class_weight='balanced', \n",
    "                         probability=True, \n",
    "                         random_state=random_state))\n",
    "        ]),\n",
    "        \"XGBoost\": XGBClassifier(\n",
    "            scale_pos_weight=(len(y) - sum(y)) / sum(y),\n",
    "            random_state=random_state,\n",
    "            eval_metric='logloss'\n",
    "        )\n",
    "    }\n",
    "\n",
    "    # 3. Кросс-валидация с LeaveOneGroupOut\n",
    "    logo = LeaveOneGroupOut()\n",
    "    model_recalls = []\n",
    "\n",
    "    for name, model in models.items():\n",
    "        recall_scores = []\n",
    "        \n",
    "        for train_idx, val_idx in logo.split(X, y, groups=groups):\n",
    "            model.fit(X[train_idx], y[train_idx])\n",
    "            y_pred = model.predict(X[val_idx])\n",
    "            recall = recall_score(y[val_idx], y_pred, pos_label=target_class)\n",
    "            recall_scores.append(recall)\n",
    "        \n",
    "        mean_recall = np.mean(recall_scores)\n",
    "        std_recall = np.std(recall_scores)\n",
    "        model_recalls.append((name, mean_recall, std_recall, model))\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"{name:<20} | Recall (class {target_class}): {mean_recall:.3f} ± {std_recall:.3f}\")\n",
    "\n",
    "        model.fit(X,y)\n",
    "    # # 4. Выбор и финальное обучение лучшей модели\n",
    "    # best_model_info = max(model_recalls, key=lambda x: x[1])\n",
    "    # best_model = best_model_info[3].fit(X, y)  # Обучаем на всех данных\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_on_test(train_matrix, test_matrix):\n",
    "    models = train_best_model_by_recall(train_matrix)\n",
    "    for name, model in models.items():\n",
    "        print(f'Model: {name}')\n",
    "        matrix_test = np.load(test_matrix)\n",
    "        N_test = matrix_test.shape[0]  # Длина массива\n",
    "        sub_num_test = N_test // 5\n",
    "\n",
    "        labels_test = np.zeros(N_test, dtype=int)  # Создаем массив из нулей\n",
    "        labels_test[3::5] = 1  # Каждый 4-й элемен\n",
    "        print(classification_report(labels_test, model.predict(matrix_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportional_train_matrix = '/home/aaanpilov/diploma/project/numpy_matrixes/ranks_matrix/proportional/raw_HC/auc.npy'\n",
    "proportional_test_matrix = '/home/aaanpilov/diploma/project/numpy_matrixes/ranks_matrix/proportional/raw_test/auc.npy'\n",
    "\n",
    "reduced_train_matrix = '/home/aaanpilov/diploma/project/numpy_matrixes/ranks_matrix/reduced_ranks/auc_raw_HC.npy'\n",
    "reduced_test_matrix = '/home/aaanpilov/diploma/project/numpy_matrixes/ranks_matrix/reduced_ranks/auc_test.npy'\n",
    "\n",
    "ranks_train_matrix = '/home/aaanpilov/diploma/project/numpy_matrixes/ranks_matrix/raw_HC/auc.npy'\n",
    "ranks_test_matrix = '/home/aaanpilov/diploma/project/numpy_matrixes/ranks_matrix/raw_test/auc.npy'\n",
    "\n",
    "# train_matrix  = '/home/aaanpilov/diploma/project/numpy_matrixes/average_stimulus/HC/max.npy'\n",
    "# test_matrix = '/home/aaanpilov/diploma/project/numpy_matrixes/average_stimulus/test/max.npy'\n",
    "\n",
    "train_matrix = proportional_train_matrix\n",
    "test_matrix = proportional_test_matrix \n",
    "\n",
    "# train_matrix = ranks_train_matrix\n",
    "# test_matrix = ranks_test_matrix\n",
    "\n",
    "# train_matrix = reduced_train_matrix\n",
    "# test_matrix = reduced_test_matrix\n",
    "\n",
    "train_matrix, test_matrix = test_matrix, train_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_different_params(train_matrix_base, test_matrix_base):\n",
    "    for func_name in funcs.keys():\n",
    "        if func_name in ('max_min', 'min'):\n",
    "            continue\n",
    "        train_matrix = os.path.join(train_matrix_base, func_name + '.npy')\n",
    "        test_matrix = os.path.join(test_matrix_base, func_name + '.npy')\n",
    "\n",
    "        print('-'*10 + func_name + '-'*10)\n",
    "        train_and_predict_on_test(train_matrix, test_matrix)\n",
    "        print('-' * 100, sep='\\n\\n\\n\\n\\n\\n')\n",
    "\n",
    "        train_matrix, test_matrix = test_matrix, train_matrix\n",
    "\n",
    "        print('TEST AND TRAIN DATA REVERT' * 10)\n",
    "        print('-'*10 + func_name + '-'*10)\n",
    "        train_and_predict_on_test(train_matrix, test_matrix)\n",
    "        print('-' * 100, sep='\\n\\n\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пропорциональные баллы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------auc----------\n",
      "Logistic Regression  | Recall (class 1): 0.433 ± 0.200\n",
      "Random Forest        | Recall (class 1): 0.200 ± 0.163\n",
      "SVM                  | Recall (class 1): 0.133 ± 0.125\n",
      "XGBoost              | Recall (class 1): 0.333 ± 0.211\n",
      "Model: Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.93        40\n",
      "           1       0.75      0.60      0.67        10\n",
      "\n",
      "    accuracy                           0.88        50\n",
      "   macro avg       0.83      0.77      0.80        50\n",
      "weighted avg       0.87      0.88      0.87        50\n",
      "\n",
      "Model: Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      1.00      0.92        40\n",
      "           1       1.00      0.30      0.46        10\n",
      "\n",
      "    accuracy                           0.86        50\n",
      "   macro avg       0.93      0.65      0.69        50\n",
      "weighted avg       0.88      0.86      0.83        50\n",
      "\n",
      "Model: SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.93      0.89        40\n",
      "           1       0.57      0.40      0.47        10\n",
      "\n",
      "    accuracy                           0.82        50\n",
      "   macro avg       0.72      0.66      0.68        50\n",
      "weighted avg       0.80      0.82      0.81        50\n",
      "\n",
      "Model: XGBoost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.95      0.89        40\n",
      "           1       0.60      0.30      0.40        10\n",
      "\n",
      "    accuracy                           0.82        50\n",
      "   macro avg       0.72      0.62      0.65        50\n",
      "weighted avg       0.80      0.82      0.80        50\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "TEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERT\n",
      "----------auc----------\n",
      "Logistic Regression  | Recall (class 1): 0.600 ± 0.133\n",
      "Random Forest        | Recall (class 1): 0.333 ± 0.211\n",
      "SVM                  | Recall (class 1): 0.000 ± 0.000\n",
      "XGBoost              | Recall (class 1): 0.467 ± 0.163\n",
      "Model: Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.90      0.88        68\n",
      "           1       0.53      0.47      0.50        17\n",
      "\n",
      "    accuracy                           0.81        85\n",
      "   macro avg       0.70      0.68      0.69        85\n",
      "weighted avg       0.80      0.81      0.81        85\n",
      "\n",
      "Model: Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.97      0.91        68\n",
      "           1       0.75      0.35      0.48        17\n",
      "\n",
      "    accuracy                           0.85        85\n",
      "   macro avg       0.80      0.66      0.70        85\n",
      "weighted avg       0.84      0.85      0.82        85\n",
      "\n",
      "Model: SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.99      0.89        68\n",
      "           1       0.67      0.12      0.20        17\n",
      "\n",
      "    accuracy                           0.81        85\n",
      "   macro avg       0.74      0.55      0.55        85\n",
      "weighted avg       0.79      0.81      0.75        85\n",
      "\n",
      "Model: XGBoost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.93      0.91        68\n",
      "           1       0.64      0.53      0.58        17\n",
      "\n",
      "    accuracy                           0.85        85\n",
      "   macro avg       0.77      0.73      0.74        85\n",
      "weighted avg       0.84      0.85      0.84        85\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------max----------\n",
      "Logistic Regression  | Recall (class 1): 0.633 ± 0.163\n",
      "Random Forest        | Recall (class 1): 0.333 ± 0.183\n",
      "SVM                  | Recall (class 1): 0.533 ± 0.194\n",
      "XGBoost              | Recall (class 1): 0.467 ± 0.125\n",
      "Model: Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.95      0.89        40\n",
      "           1       0.60      0.30      0.40        10\n",
      "\n",
      "    accuracy                           0.82        50\n",
      "   macro avg       0.72      0.62      0.65        50\n",
      "weighted avg       0.80      0.82      0.80        50\n",
      "\n",
      "Model: Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      1.00      0.91        40\n",
      "           1       1.00      0.20      0.33        10\n",
      "\n",
      "    accuracy                           0.84        50\n",
      "   macro avg       0.92      0.60      0.62        50\n",
      "weighted avg       0.87      0.84      0.79        50\n",
      "\n",
      "Model: SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.97      0.92        40\n",
      "           1       0.80      0.40      0.53        10\n",
      "\n",
      "    accuracy                           0.86        50\n",
      "   macro avg       0.83      0.69      0.73        50\n",
      "weighted avg       0.85      0.86      0.84        50\n",
      "\n",
      "Model: XGBoost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.97      0.90        40\n",
      "           1       0.67      0.20      0.31        10\n",
      "\n",
      "    accuracy                           0.82        50\n",
      "   macro avg       0.75      0.59      0.60        50\n",
      "weighted avg       0.80      0.82      0.78        50\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "TEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERT\n",
      "----------max----------\n",
      "Logistic Regression  | Recall (class 1): 0.467 ± 0.163\n",
      "Random Forest        | Recall (class 1): 0.333 ± 0.000\n",
      "SVM                  | Recall (class 1): 0.000 ± 0.000\n",
      "XGBoost              | Recall (class 1): 0.600 ± 0.249\n",
      "Model: Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.90      0.90        68\n",
      "           1       0.59      0.59      0.59        17\n",
      "\n",
      "    accuracy                           0.84        85\n",
      "   macro avg       0.74      0.74      0.74        85\n",
      "weighted avg       0.84      0.84      0.84        85\n",
      "\n",
      "Model: Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.99      0.92        68\n",
      "           1       0.86      0.35      0.50        17\n",
      "\n",
      "    accuracy                           0.86        85\n",
      "   macro avg       0.86      0.67      0.71        85\n",
      "weighted avg       0.86      0.86      0.83        85\n",
      "\n",
      "Model: SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      1.00      0.90        68\n",
      "           1       1.00      0.12      0.21        17\n",
      "\n",
      "    accuracy                           0.82        85\n",
      "   macro avg       0.91      0.56      0.56        85\n",
      "weighted avg       0.86      0.82      0.76        85\n",
      "\n",
      "Model: XGBoost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94        68\n",
      "           1       0.76      0.76      0.76        17\n",
      "\n",
      "    accuracy                           0.91        85\n",
      "   macro avg       0.85      0.85      0.85        85\n",
      "weighted avg       0.91      0.91      0.91        85\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_matrix_base = '/home/aaanpilov/diploma/project/numpy_matrixes/ranks_matrix/proportional/raw_HC'\n",
    "test_matrix_base = '/home/aaanpilov/diploma/project/numpy_matrixes/ranks_matrix/proportional/raw_test'\n",
    "\n",
    "train_different_params(train_matrix_base, test_matrix_base)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Баллы по 1,2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------auc----------\n",
      "Logistic Regression  | Recall (class 1): 0.500 ± 0.149\n",
      "Random Forest        | Recall (class 1): 0.200 ± 0.163\n",
      "SVM                  | Recall (class 1): 0.367 ± 0.163\n",
      "XGBoost              | Recall (class 1): 0.433 ± 0.226\n",
      "Model: Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.95      0.89        40\n",
      "           1       0.60      0.30      0.40        10\n",
      "\n",
      "    accuracy                           0.82        50\n",
      "   macro avg       0.72      0.62      0.65        50\n",
      "weighted avg       0.80      0.82      0.80        50\n",
      "\n",
      "Model: Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      1.00      0.90        40\n",
      "           1       1.00      0.10      0.18        10\n",
      "\n",
      "    accuracy                           0.82        50\n",
      "   macro avg       0.91      0.55      0.54        50\n",
      "weighted avg       0.85      0.82      0.76        50\n",
      "\n",
      "Model: SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      1.00      0.91        40\n",
      "           1       1.00      0.20      0.33        10\n",
      "\n",
      "    accuracy                           0.84        50\n",
      "   macro avg       0.92      0.60      0.62        50\n",
      "weighted avg       0.87      0.84      0.79        50\n",
      "\n",
      "Model: XGBoost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.95      0.90        40\n",
      "           1       0.67      0.40      0.50        10\n",
      "\n",
      "    accuracy                           0.84        50\n",
      "   macro avg       0.77      0.68      0.70        50\n",
      "weighted avg       0.82      0.84      0.82        50\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "TEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERT\n",
      "----------auc----------\n",
      "Logistic Regression  | Recall (class 1): 0.533 ± 0.163\n",
      "Random Forest        | Recall (class 1): 0.333 ± 0.211\n",
      "SVM                  | Recall (class 1): 0.267 ± 0.327\n",
      "XGBoost              | Recall (class 1): 0.467 ± 0.163\n",
      "Model: Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.94      0.92        68\n",
      "           1       0.71      0.59      0.65        17\n",
      "\n",
      "    accuracy                           0.87        85\n",
      "   macro avg       0.81      0.76      0.78        85\n",
      "weighted avg       0.86      0.87      0.87        85\n",
      "\n",
      "Model: Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.97      0.90        68\n",
      "           1       0.67      0.24      0.35        17\n",
      "\n",
      "    accuracy                           0.82        85\n",
      "   macro avg       0.75      0.60      0.62        85\n",
      "weighted avg       0.80      0.82      0.79        85\n",
      "\n",
      "Model: SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.97      0.91        68\n",
      "           1       0.75      0.35      0.48        17\n",
      "\n",
      "    accuracy                           0.85        85\n",
      "   macro avg       0.80      0.66      0.70        85\n",
      "weighted avg       0.84      0.85      0.82        85\n",
      "\n",
      "Model: XGBoost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.93      0.90        68\n",
      "           1       0.62      0.47      0.53        17\n",
      "\n",
      "    accuracy                           0.84        85\n",
      "   macro avg       0.75      0.70      0.72        85\n",
      "weighted avg       0.82      0.84      0.83        85\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------max----------\n",
      "Logistic Regression  | Recall (class 1): 0.567 ± 0.082\n",
      "Random Forest        | Recall (class 1): 0.300 ± 0.163\n",
      "SVM                  | Recall (class 1): 0.500 ± 0.105\n",
      "XGBoost              | Recall (class 1): 0.600 ± 0.133\n",
      "Model: Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.93      0.90        40\n",
      "           1       0.62      0.50      0.56        10\n",
      "\n",
      "    accuracy                           0.84        50\n",
      "   macro avg       0.75      0.71      0.73        50\n",
      "weighted avg       0.83      0.84      0.83        50\n",
      "\n",
      "Model: Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.97      0.90        40\n",
      "           1       0.67      0.20      0.31        10\n",
      "\n",
      "    accuracy                           0.82        50\n",
      "   macro avg       0.75      0.59      0.60        50\n",
      "weighted avg       0.80      0.82      0.78        50\n",
      "\n",
      "Model: SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.97      0.93        40\n",
      "           1       0.83      0.50      0.62        10\n",
      "\n",
      "    accuracy                           0.88        50\n",
      "   macro avg       0.86      0.74      0.78        50\n",
      "weighted avg       0.88      0.88      0.87        50\n",
      "\n",
      "Model: XGBoost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.97      0.90        40\n",
      "           1       0.67      0.20      0.31        10\n",
      "\n",
      "    accuracy                           0.82        50\n",
      "   macro avg       0.75      0.59      0.60        50\n",
      "weighted avg       0.80      0.82      0.78        50\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "TEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERT\n",
      "----------max----------\n",
      "Logistic Regression  | Recall (class 1): 0.667 ± 0.211\n",
      "Random Forest        | Recall (class 1): 0.267 ± 0.133\n",
      "SVM                  | Recall (class 1): 0.067 ± 0.133\n",
      "XGBoost              | Recall (class 1): 0.533 ± 0.163\n",
      "Model: Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.93        68\n",
      "           1       0.73      0.65      0.69        17\n",
      "\n",
      "    accuracy                           0.88        85\n",
      "   macro avg       0.82      0.79      0.81        85\n",
      "weighted avg       0.88      0.88      0.88        85\n",
      "\n",
      "Model: Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.97      0.90        68\n",
      "           1       0.67      0.24      0.35        17\n",
      "\n",
      "    accuracy                           0.82        85\n",
      "   macro avg       0.75      0.60      0.62        85\n",
      "weighted avg       0.80      0.82      0.79        85\n",
      "\n",
      "Model: SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.97      0.93        68\n",
      "           1       0.82      0.53      0.64        17\n",
      "\n",
      "    accuracy                           0.88        85\n",
      "   macro avg       0.86      0.75      0.79        85\n",
      "weighted avg       0.88      0.88      0.87        85\n",
      "\n",
      "Model: XGBoost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.90      0.90        68\n",
      "           1       0.59      0.59      0.59        17\n",
      "\n",
      "    accuracy                           0.84        85\n",
      "   macro avg       0.74      0.74      0.74        85\n",
      "weighted avg       0.84      0.84      0.84        85\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "reduced_train_matrix_base = '/home/aaanpilov/diploma/project/numpy_matrixes/ranks_matrix/reduced_ranks/raw_HC'\n",
    "reduced_test_matrix_base = '/home/aaanpilov/diploma/project/numpy_matrixes/ranks_matrix/reduced_ranks/raw_test'\n",
    "\n",
    "train_different_params(reduced_train_matrix_base, reduced_test_matrix_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Просто баллы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------auc----------\n",
      "Logistic Regression  | Recall (class 1): 0.647 ± 0.478\n",
      "Random Forest        | Recall (class 1): 0.294 ± 0.456\n",
      "SVM                  | Recall (class 1): 0.588 ± 0.492\n",
      "XGBoost              | Recall (class 1): 0.588 ± 0.492\n",
      "Model: Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.95      0.90        40\n",
      "           1       0.67      0.40      0.50        10\n",
      "\n",
      "    accuracy                           0.84        50\n",
      "   macro avg       0.77      0.68      0.70        50\n",
      "weighted avg       0.82      0.84      0.82        50\n",
      "\n",
      "Model: Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      1.00      0.90        40\n",
      "           1       1.00      0.10      0.18        10\n",
      "\n",
      "    accuracy                           0.82        50\n",
      "   macro avg       0.91      0.55      0.54        50\n",
      "weighted avg       0.85      0.82      0.76        50\n",
      "\n",
      "Model: SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.97      0.91        40\n",
      "           1       0.75      0.30      0.43        10\n",
      "\n",
      "    accuracy                           0.84        50\n",
      "   macro avg       0.80      0.64      0.67        50\n",
      "weighted avg       0.83      0.84      0.81        50\n",
      "\n",
      "Model: XGBoost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.97      0.92        40\n",
      "           1       0.80      0.40      0.53        10\n",
      "\n",
      "    accuracy                           0.86        50\n",
      "   macro avg       0.83      0.69      0.73        50\n",
      "weighted avg       0.85      0.86      0.84        50\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "TEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERT\n",
      "----------auc----------\n",
      "Logistic Regression  | Recall (class 1): 0.400 ± 0.490\n",
      "Random Forest        | Recall (class 1): 0.200 ± 0.400\n",
      "SVM                  | Recall (class 1): 0.400 ± 0.490\n",
      "XGBoost              | Recall (class 1): 0.400 ± 0.490\n",
      "Model: Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94        68\n",
      "           1       0.76      0.76      0.76        17\n",
      "\n",
      "    accuracy                           0.91        85\n",
      "   macro avg       0.85      0.85      0.85        85\n",
      "weighted avg       0.91      0.91      0.91        85\n",
      "\n",
      "Model: Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.97      0.92        68\n",
      "           1       0.78      0.41      0.54        17\n",
      "\n",
      "    accuracy                           0.86        85\n",
      "   macro avg       0.82      0.69      0.73        85\n",
      "weighted avg       0.85      0.86      0.84        85\n",
      "\n",
      "Model: SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.97      0.94        68\n",
      "           1       0.83      0.59      0.69        17\n",
      "\n",
      "    accuracy                           0.89        85\n",
      "   macro avg       0.87      0.78      0.81        85\n",
      "weighted avg       0.89      0.89      0.89        85\n",
      "\n",
      "Model: XGBoost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.91      0.90        68\n",
      "           1       0.60      0.53      0.56        17\n",
      "\n",
      "    accuracy                           0.84        85\n",
      "   macro avg       0.74      0.72      0.73        85\n",
      "weighted avg       0.83      0.84      0.83        85\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------max----------\n",
      "Logistic Regression  | Recall (class 1): 0.706 ± 0.456\n",
      "Random Forest        | Recall (class 1): 0.588 ± 0.492\n",
      "SVM                  | Recall (class 1): 0.588 ± 0.492\n",
      "XGBoost              | Recall (class 1): 0.765 ± 0.424\n",
      "Model: Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.93      0.89        40\n",
      "           1       0.57      0.40      0.47        10\n",
      "\n",
      "    accuracy                           0.82        50\n",
      "   macro avg       0.72      0.66      0.68        50\n",
      "weighted avg       0.80      0.82      0.81        50\n",
      "\n",
      "Model: Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.97      0.89        40\n",
      "           1       0.50      0.10      0.17        10\n",
      "\n",
      "    accuracy                           0.80        50\n",
      "   macro avg       0.66      0.54      0.53        50\n",
      "weighted avg       0.75      0.80      0.74        50\n",
      "\n",
      "Model: SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      1.00      0.93        40\n",
      "           1       1.00      0.40      0.57        10\n",
      "\n",
      "    accuracy                           0.88        50\n",
      "   macro avg       0.93      0.70      0.75        50\n",
      "weighted avg       0.90      0.88      0.86        50\n",
      "\n",
      "Model: XGBoost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.97      0.91        40\n",
      "           1       0.75      0.30      0.43        10\n",
      "\n",
      "    accuracy                           0.84        50\n",
      "   macro avg       0.80      0.64      0.67        50\n",
      "weighted avg       0.83      0.84      0.81        50\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "TEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERT\n",
      "----------max----------\n",
      "Logistic Regression  | Recall (class 1): 0.400 ± 0.490\n",
      "Random Forest        | Recall (class 1): 0.100 ± 0.300\n",
      "SVM                  | Recall (class 1): 0.100 ± 0.300\n",
      "XGBoost              | Recall (class 1): 0.400 ± 0.490\n",
      "Model: Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.96      0.94        68\n",
      "           1       0.80      0.71      0.75        17\n",
      "\n",
      "    accuracy                           0.91        85\n",
      "   macro avg       0.86      0.83      0.85        85\n",
      "weighted avg       0.90      0.91      0.90        85\n",
      "\n",
      "Model: Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.99      0.92        68\n",
      "           1       0.88      0.41      0.56        17\n",
      "\n",
      "    accuracy                           0.87        85\n",
      "   macro avg       0.87      0.70      0.74        85\n",
      "weighted avg       0.87      0.87      0.85        85\n",
      "\n",
      "Model: SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.97      0.95        68\n",
      "           1       0.86      0.71      0.77        17\n",
      "\n",
      "    accuracy                           0.92        85\n",
      "   macro avg       0.89      0.84      0.86        85\n",
      "weighted avg       0.92      0.92      0.91        85\n",
      "\n",
      "Model: XGBoost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.93        68\n",
      "           1       0.73      0.65      0.69        17\n",
      "\n",
      "    accuracy                           0.88        85\n",
      "   macro avg       0.82      0.79      0.81        85\n",
      "weighted avg       0.88      0.88      0.88        85\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "ranks_train_matrix = '/home/aaanpilov/diploma/project/numpy_matrixes/ranks_matrix/raw_HC'\n",
    "ranks_test_matrix = '/home/aaanpilov/diploma/project/numpy_matrixes/ranks_matrix/raw_test'\n",
    "\n",
    "train_different_params(ranks_train_matrix, ranks_test_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------auc----------\n",
      "Logistic Regression  | Recall (class 1): 0.706 ± 0.456\n",
      "Random Forest        | Recall (class 1): 0.353 ± 0.478\n",
      "SVM                  | Recall (class 1): 0.588 ± 0.492\n",
      "XGBoost              | Recall (class 1): 0.647 ± 0.478\n",
      "Model: Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95        40\n",
      "           1       0.80      0.80      0.80        10\n",
      "\n",
      "    accuracy                           0.92        50\n",
      "   macro avg       0.88      0.88      0.88        50\n",
      "weighted avg       0.92      0.92      0.92        50\n",
      "\n",
      "Model: Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.97      0.91        40\n",
      "           1       0.75      0.30      0.43        10\n",
      "\n",
      "    accuracy                           0.84        50\n",
      "   macro avg       0.80      0.64      0.67        50\n",
      "weighted avg       0.83      0.84      0.81        50\n",
      "\n",
      "Model: SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.93        40\n",
      "           1       0.75      0.60      0.67        10\n",
      "\n",
      "    accuracy                           0.88        50\n",
      "   macro avg       0.83      0.77      0.80        50\n",
      "weighted avg       0.87      0.88      0.87        50\n",
      "\n",
      "Model: XGBoost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88        40\n",
      "           1       0.50      0.50      0.50        10\n",
      "\n",
      "    accuracy                           0.80        50\n",
      "   macro avg       0.69      0.69      0.69        50\n",
      "weighted avg       0.80      0.80      0.80        50\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "TEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERT\n",
      "----------auc----------\n",
      "Logistic Regression  | Recall (class 1): 0.700 ± 0.458\n",
      "Random Forest        | Recall (class 1): 0.200 ± 0.400\n",
      "SVM                  | Recall (class 1): 0.400 ± 0.490\n",
      "XGBoost              | Recall (class 1): 0.600 ± 0.490\n",
      "Model: Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.97      0.95        68\n",
      "           1       0.86      0.71      0.77        17\n",
      "\n",
      "    accuracy                           0.92        85\n",
      "   macro avg       0.89      0.84      0.86        85\n",
      "weighted avg       0.92      0.92      0.91        85\n",
      "\n",
      "Model: Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.99      0.91        68\n",
      "           1       0.83      0.29      0.43        17\n",
      "\n",
      "    accuracy                           0.85        85\n",
      "   macro avg       0.84      0.64      0.67        85\n",
      "weighted avg       0.85      0.85      0.82        85\n",
      "\n",
      "Model: SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.97      0.92        68\n",
      "           1       0.80      0.47      0.59        17\n",
      "\n",
      "    accuracy                           0.87        85\n",
      "   macro avg       0.84      0.72      0.76        85\n",
      "weighted avg       0.86      0.87      0.86        85\n",
      "\n",
      "Model: XGBoost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.94      0.92        68\n",
      "           1       0.71      0.59      0.65        17\n",
      "\n",
      "    accuracy                           0.87        85\n",
      "   macro avg       0.81      0.76      0.78        85\n",
      "weighted avg       0.86      0.87      0.87        85\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------max----------\n",
      "Logistic Regression  | Recall (class 1): 0.706 ± 0.456\n",
      "Random Forest        | Recall (class 1): 0.471 ± 0.499\n",
      "SVM                  | Recall (class 1): 0.647 ± 0.478\n",
      "XGBoost              | Recall (class 1): 0.647 ± 0.478\n",
      "Model: Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.97      0.92        40\n",
      "           1       0.80      0.40      0.53        10\n",
      "\n",
      "    accuracy                           0.86        50\n",
      "   macro avg       0.83      0.69      0.73        50\n",
      "weighted avg       0.85      0.86      0.84        50\n",
      "\n",
      "Model: Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.97      0.91        40\n",
      "           1       0.75      0.30      0.43        10\n",
      "\n",
      "    accuracy                           0.84        50\n",
      "   macro avg       0.80      0.64      0.67        50\n",
      "weighted avg       0.83      0.84      0.81        50\n",
      "\n",
      "Model: SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.97      0.92        40\n",
      "           1       0.80      0.40      0.53        10\n",
      "\n",
      "    accuracy                           0.86        50\n",
      "   macro avg       0.83      0.69      0.73        50\n",
      "weighted avg       0.85      0.86      0.84        50\n",
      "\n",
      "Model: XGBoost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.90      0.87        40\n",
      "           1       0.43      0.30      0.35        10\n",
      "\n",
      "    accuracy                           0.78        50\n",
      "   macro avg       0.63      0.60      0.61        50\n",
      "weighted avg       0.76      0.78      0.76        50\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "TEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERT\n",
      "----------max----------\n",
      "Logistic Regression  | Recall (class 1): 0.600 ± 0.490\n",
      "Random Forest        | Recall (class 1): 0.000 ± 0.000\n",
      "SVM                  | Recall (class 1): 0.400 ± 0.490\n",
      "XGBoost              | Recall (class 1): 0.200 ± 0.400\n",
      "Model: Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.93        68\n",
      "           1       0.68      0.76      0.72        17\n",
      "\n",
      "    accuracy                           0.88        85\n",
      "   macro avg       0.81      0.84      0.82        85\n",
      "weighted avg       0.89      0.88      0.88        85\n",
      "\n",
      "Model: Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.94        68\n",
      "           1       1.00      0.47      0.64        17\n",
      "\n",
      "    accuracy                           0.89        85\n",
      "   macro avg       0.94      0.74      0.79        85\n",
      "weighted avg       0.91      0.89      0.88        85\n",
      "\n",
      "Model: SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.97      0.94        68\n",
      "           1       0.85      0.65      0.73        17\n",
      "\n",
      "    accuracy                           0.91        85\n",
      "   macro avg       0.88      0.81      0.84        85\n",
      "weighted avg       0.90      0.91      0.90        85\n",
      "\n",
      "Model: XGBoost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.94      0.91        68\n",
      "           1       0.67      0.47      0.55        17\n",
      "\n",
      "    accuracy                           0.85        85\n",
      "   macro avg       0.77      0.71      0.73        85\n",
      "weighted avg       0.83      0.85      0.84        85\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_matrix_base = '/home/aaanpilov/diploma/project/numpy_matrixes/average_stimulus/HC'\n",
    "test_matrix_base = '/home/aaanpilov/diploma/project/numpy_matrixes/average_stimulus/test'\n",
    "\n",
    "train_different_params(train_matrix_base, test_matrix_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------auc----------\n",
      "Logistic Regression  | Recall (class 1): 0.533 ± 0.125\n",
      "Random Forest        | Recall (class 1): 0.133 ± 0.125\n",
      "SVM                  | Recall (class 1): 0.300 ± 0.221\n",
      "XGBoost              | Recall (class 1): 0.300 ± 0.194\n",
      "Model: Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.95      0.90        40\n",
      "           1       0.67      0.40      0.50        10\n",
      "\n",
      "    accuracy                           0.84        50\n",
      "   macro avg       0.77      0.68      0.70        50\n",
      "weighted avg       0.82      0.84      0.82        50\n",
      "\n",
      "Model: Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      1.00      0.90        40\n",
      "           1       1.00      0.10      0.18        10\n",
      "\n",
      "    accuracy                           0.82        50\n",
      "   macro avg       0.91      0.55      0.54        50\n",
      "weighted avg       0.85      0.82      0.76        50\n",
      "\n",
      "Model: SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.97      0.91        40\n",
      "           1       0.75      0.30      0.43        10\n",
      "\n",
      "    accuracy                           0.84        50\n",
      "   macro avg       0.80      0.64      0.67        50\n",
      "weighted avg       0.83      0.84      0.81        50\n",
      "\n",
      "Model: XGBoost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.95      0.92        40\n",
      "           1       0.71      0.50      0.59        10\n",
      "\n",
      "    accuracy                           0.86        50\n",
      "   macro avg       0.80      0.72      0.75        50\n",
      "weighted avg       0.85      0.86      0.85        50\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "TEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERT\n",
      "----------auc----------\n",
      "Logistic Regression  | Recall (class 1): 0.600 ± 0.133\n",
      "Random Forest        | Recall (class 1): 0.267 ± 0.133\n",
      "SVM                  | Recall (class 1): 0.200 ± 0.267\n",
      "XGBoost              | Recall (class 1): 0.467 ± 0.267\n",
      "Model: Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.93      0.93        68\n",
      "           1       0.72      0.76      0.74        17\n",
      "\n",
      "    accuracy                           0.89        85\n",
      "   macro avg       0.83      0.85      0.84        85\n",
      "weighted avg       0.90      0.89      0.90        85\n",
      "\n",
      "Model: Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.96      0.89        68\n",
      "           1       0.57      0.24      0.33        17\n",
      "\n",
      "    accuracy                           0.81        85\n",
      "   macro avg       0.70      0.60      0.61        85\n",
      "weighted avg       0.78      0.81      0.78        85\n",
      "\n",
      "Model: SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.97      0.91        68\n",
      "           1       0.75      0.35      0.48        17\n",
      "\n",
      "    accuracy                           0.85        85\n",
      "   macro avg       0.80      0.66      0.70        85\n",
      "weighted avg       0.84      0.85      0.82        85\n",
      "\n",
      "Model: XGBoost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.84      0.87        68\n",
      "           1       0.50      0.65      0.56        17\n",
      "\n",
      "    accuracy                           0.80        85\n",
      "   macro avg       0.70      0.74      0.72        85\n",
      "weighted avg       0.82      0.80      0.81        85\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------max----------\n",
      "Logistic Regression  | Recall (class 1): 0.667 ± 0.149\n",
      "Random Forest        | Recall (class 1): 0.467 ± 0.067\n",
      "SVM                  | Recall (class 1): 0.500 ± 0.105\n",
      "XGBoost              | Recall (class 1): 0.667 ± 0.149\n",
      "Model: Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.93      0.89        40\n",
      "           1       0.57      0.40      0.47        10\n",
      "\n",
      "    accuracy                           0.82        50\n",
      "   macro avg       0.72      0.66      0.68        50\n",
      "weighted avg       0.80      0.82      0.81        50\n",
      "\n",
      "Model: Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      1.00      0.90        40\n",
      "           1       1.00      0.10      0.18        10\n",
      "\n",
      "    accuracy                           0.82        50\n",
      "   macro avg       0.91      0.55      0.54        50\n",
      "weighted avg       0.85      0.82      0.76        50\n",
      "\n",
      "Model: SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      1.00      0.92        40\n",
      "           1       1.00      0.30      0.46        10\n",
      "\n",
      "    accuracy                           0.86        50\n",
      "   macro avg       0.93      0.65      0.69        50\n",
      "weighted avg       0.88      0.86      0.83        50\n",
      "\n",
      "Model: XGBoost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.95      0.87        40\n",
      "           1       0.33      0.10      0.15        10\n",
      "\n",
      "    accuracy                           0.78        50\n",
      "   macro avg       0.57      0.53      0.51        50\n",
      "weighted avg       0.71      0.78      0.73        50\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "TEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERTTEST AND TRAIN DATA REVERT\n",
      "----------max----------\n",
      "Logistic Regression  | Recall (class 1): 0.600 ± 0.249\n",
      "Random Forest        | Recall (class 1): 0.267 ± 0.133\n",
      "SVM                  | Recall (class 1): 0.067 ± 0.133\n",
      "XGBoost              | Recall (class 1): 0.467 ± 0.267\n",
      "Model: Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.94      0.92        68\n",
      "           1       0.71      0.59      0.65        17\n",
      "\n",
      "    accuracy                           0.87        85\n",
      "   macro avg       0.81      0.76      0.78        85\n",
      "weighted avg       0.86      0.87      0.87        85\n",
      "\n",
      "Model: Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      1.00      0.92        68\n",
      "           1       1.00      0.29      0.45        17\n",
      "\n",
      "    accuracy                           0.86        85\n",
      "   macro avg       0.93      0.65      0.69        85\n",
      "weighted avg       0.88      0.86      0.83        85\n",
      "\n",
      "Model: SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.99      0.92        68\n",
      "           1       0.88      0.41      0.56        17\n",
      "\n",
      "    accuracy                           0.87        85\n",
      "   macro avg       0.87      0.70      0.74        85\n",
      "weighted avg       0.87      0.87      0.85        85\n",
      "\n",
      "Model: XGBoost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.91      0.91        68\n",
      "           1       0.65      0.65      0.65        17\n",
      "\n",
      "    accuracy                           0.86        85\n",
      "   macro avg       0.78      0.78      0.78        85\n",
      "weighted avg       0.86      0.86      0.86        85\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_matrix_base = '/home/aaanpilov/diploma/project/numpy_matrixes/ranks_matrix/prizes/HC'\n",
    "test_matrix_base = '/home/aaanpilov/diploma/project/numpy_matrixes/ranks_matrix/prizes/test'\n",
    "\n",
    "train_different_params(train_matrix_base, test_matrix_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------auc----------\n",
      "Logistic Regression  | Recall (class 1): 0.706 ± 0.456\n",
      "Random Forest        | Recall (class 1): 0.353 ± 0.478\n",
      "SVM                  | Recall (class 1): 0.588 ± 0.492\n",
      "XGBoost              | Recall (class 1): 0.647 ± 0.478\n",
      "Model: Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.82      0.81        68\n",
      "           1       0.20      0.18      0.19        17\n",
      "\n",
      "    accuracy                           0.69        85\n",
      "   macro avg       0.50      0.50      0.50        85\n",
      "weighted avg       0.68      0.69      0.69        85\n",
      "\n",
      "Model: Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.87      0.84        68\n",
      "           1       0.25      0.18      0.21        17\n",
      "\n",
      "    accuracy                           0.73        85\n",
      "   macro avg       0.53      0.52      0.52        85\n",
      "weighted avg       0.70      0.73      0.71        85\n",
      "\n",
      "Model: SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.84      0.83        68\n",
      "           1       0.27      0.24      0.25        17\n",
      "\n",
      "    accuracy                           0.72        85\n",
      "   macro avg       0.54      0.54      0.54        85\n",
      "weighted avg       0.70      0.72      0.71        85\n",
      "\n",
      "Model: XGBoost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.79      0.80        68\n",
      "           1       0.22      0.24      0.23        17\n",
      "\n",
      "    accuracy                           0.68        85\n",
      "   macro avg       0.51      0.51      0.51        85\n",
      "weighted avg       0.69      0.68      0.69        85\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------max----------\n",
      "Logistic Regression  | Recall (class 1): 0.706 ± 0.456\n",
      "Random Forest        | Recall (class 1): 0.471 ± 0.499\n",
      "SVM                  | Recall (class 1): 0.647 ± 0.478\n",
      "XGBoost              | Recall (class 1): 0.647 ± 0.478\n",
      "Model: Logistic Regression\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.87      0.83        68\n",
      "           1       0.18      0.12      0.14        17\n",
      "\n",
      "    accuracy                           0.72        85\n",
      "   macro avg       0.49      0.49      0.49        85\n",
      "weighted avg       0.67      0.72      0.69        85\n",
      "\n",
      "Model: Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.87      0.83        68\n",
      "           1       0.10      0.06      0.07        17\n",
      "\n",
      "    accuracy                           0.71        85\n",
      "   macro avg       0.44      0.46      0.45        85\n",
      "weighted avg       0.65      0.71      0.67        85\n",
      "\n",
      "Model: SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.87      0.83        68\n",
      "           1       0.10      0.06      0.07        17\n",
      "\n",
      "    accuracy                           0.71        85\n",
      "   macro avg       0.44      0.46      0.45        85\n",
      "weighted avg       0.65      0.71      0.67        85\n",
      "\n",
      "Model: XGBoost\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.85      0.82        68\n",
      "           1       0.09      0.06      0.07        17\n",
      "\n",
      "    accuracy                           0.69        85\n",
      "   macro avg       0.44      0.46      0.44        85\n",
      "weighted avg       0.65      0.69      0.67        85\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_matrix_base = '/home/aaanpilov/diploma/project/numpy_matrixes/average_stimulus/HC'\n",
    "test_matrix_base = '/home/aaanpilov/diploma/project/numpy_matrixes/average_stimulus/card_hc'\n",
    "for func_name in funcs.keys():\n",
    "    if func_name in ('max_min', 'min'):\n",
    "        continue\n",
    "    train_matrix = os.path.join(train_matrix_base, func_name + '.npy')\n",
    "    test_matrix = os.path.join(test_matrix_base, func_name + '.npy')\n",
    "\n",
    "    print('-'*10 + func_name + '-'*10)\n",
    "    train_and_predict_on_test(train_matrix, test_matrix)\n",
    "    print('-' * 100, sep='\\n\\n\\n\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
